@article{ref_paper,
  author       = {Kaitao Song and
                  Xu Tan and
                  Tao Qin and
                  Jianfeng Lu and
                  Tie{-}Yan Liu},
  title        = {{MASS:} Masked Sequence to Sequence Pre-training for Language Generation},
  journal      = {CoRR},
  volume       = {abs/1905.02450},
  year         = {2019},
  url          = {http://arxiv.org/abs/1905.02450},
  eprinttype    = {arXiv},
  eprint       = {1905.02450},
  timestamp    = {Fri, 04 Dec 2020 15:21:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1905-02450.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{base_paper,
    title = "Neural Machine Translation Methods for Translating Text to Sign Language Glosses",
    author = "Zhu, Dele  and
      Czehmann, Vera  and
      Avramidis, Eleftherios",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.700",
    doi = "10.18653/v1/2023.acl-long.700",
    pages = "12523--12541",
    abstract = "State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).",
}


@inproceedings{failed_exp,
  author    = {Artetxe, Mikel  and  Labaka, Gorka  and  Agirre, Eneko  and  Cho, Kyunghyun},
  title     = {Unsupervised neural machine translation},
  booktitle = {Proceedings of the Sixth International Conference on Learning Representations},
  month     = {April},
  year      = {2018}
}

@article{DBLP,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gen_pretrain,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{nmt,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@misc{word_reps,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{seq_to_seq,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{massive_nmt,
  title={Massive exploration of neural machine translation architectures},
  author={Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  journal={arXiv preprint arXiv:1703.03906},
  year={2017}
}

@inproceedings{data_augmenta,
    title = "Understanding Data Augmentation in Neural Machine Translation: Two Perspectives towards Generalization",
    author = "Li, Guanlin  and
      Liu, Lemao  and
      Huang, Guoping  and
      Zhu, Conghui  and
      Zhao, Tiejun",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1570",
    doi = "10.18653/v1/D19-1570",
}
