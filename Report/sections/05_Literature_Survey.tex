\section{Literature Survey or Earlier works} \label{sec: literature survey}

 The work \cite{DBLP} introduces BERT, a pre-training method for language understanding tasks. BERT utilizes a masked language modeling objective and next sentence prediction to learn contextualized word representations. It achieved state-of-the-art results on various language understanding benchmarks. The paper \cite{gen_pretrain} presents a generative pre-training approach called OpenAI GPT. It uses a transformer-based architecture and trains a language model on a large corpus of unlabeled text. OpenAI GPT achieves impressive performance on language understanding tasks and demonstrates the effectiveness of generative pre-training. The paper \cite{attention} introduces the Transformer model, which relies solely on self-attention mechanisms for sequence transduction tasks. The Transformer model achieves state-of-the-art results on machine translation tasks and eliminates the need for recurrent or convolutional layers. Neural Machine Translation by Jointly Learning to Align and Translate \cite{nmt} proposes an attention-based neural machine translation (NMT) model. It introduces the concept of attention mechanisms to align and translate source and target sentences. The attention-based NMT model significantly improves translation quality compared to traditional NMT models. Deep Contextualized Word Representations \cite{word_reps} presents a deep contextualized word representation model. ELMo generates word embeddings that capture both word-level and context-sensitive features. It achieves state-of-the-art results on various language understanding tasks, demonstrating the effectiveness of contextualized word representations. Sequence to Sequence Learning with Neural Networks \cite{seq_to_seq} introduces the sequence-to-sequence learning framework for neural networks. It demonstrates the effectiveness of using encoder-decoder architectures for various language generation tasks, including machine translation, text summarization, and conversational response generation. Massive Exploration of Neural Machine Translation Architectures \cite{massive_nmt} explores various neural machine translation (NMT) architectures and their impact on translation quality. It investigates different encoder and decoder architectures, attention mechanisms, and training techniques. The study provides insights into the design choices for NMT models and their effects on translation performance. "Neural Machine Translation Methods for Translating Text to Sign Language Glosses" \cite{base_paper} explores state-of-the-art techniques for improving machine translation of spoken language text to sign language glosses. The authors propose methods such as data augmentation, semi-supervised NMT, transfer learning, and multilingual NMT. They experiment with two German sign language corpora and achieve significant improvements in translation performance, outperforming previous work on the same test set. The best setting combines multilingual NMT with data augmentation and is also validated on an American Sign Language corpus. Data augmentation is a common technique used to address low resource conditions in machine translation. It involves adding synthetically generated data from various sources to the training dataset. This technique aims to increase the size and diversity of the data, making the model more robust and capable of handling variable appearances of the spoken language sentences. In the context of sign language gloss translation, data augmentation \cite{data_augmenta} has been explored to improve translation performance. 